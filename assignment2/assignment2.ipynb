{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries that the program needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "NumPy: 1.16.4\n",
      "Pandas: 0.25.3\n",
      "Scikit-learn: 0.23.2\n",
      "SpaCy: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"Scikit-learn:\", sklearn.__version__)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "print(\"SpaCy:\", spacy.__version__)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('brown')\n",
    "#nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data with the universal tagset, so it contains the POS tags as well, because we will need it in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The brown corpus contains english sentences, so we need to load the english version of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words(tagset='universal')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train, test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's define a function that will do the preprocessing of the brown corpus. This will convert the (str, POS) pairs of the brown corpus into a list of (Doc, POS) pairs. <br> <br>We need the Doc so we can use some special feature extractions like is_stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(data):\n",
    "    processed = []\n",
    "    for i in data:\n",
    "        sentence, postags = map(list, zip(*i))\n",
    "        processed.append((Doc(nlp.vocab, sentence), postags))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We go through the corpus and apply our function, after that we split the data into train and test using a 80%-20% split. <br><br> We need to use separate data for the training and testing so our model won't overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45872\n",
      "Test size: 11468\n"
     ]
    }
   ],
   "source": [
    "data = process_corpus(brown.tagged_sents(tagset='universal'))\n",
    "data_size = len(data)\n",
    "train_size = int(data_size * 0.8)\n",
    "# Split\n",
    "train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature extraction functions. We use features from the Doc and from the str. To use functions like \".lower()\" we need to convert the Doc into str, we can do this with the \".text\" property of the Doc. <br><br> We return \"BOS\" if this is the first word of the sentence and \"EOS\" if this is the last word of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2features(tokens):\n",
    "    def single_token2feature(tokens, i):\n",
    "        word = tokens[i].text\n",
    "\n",
    "        features = {\n",
    "            #'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'is_stop': tokens[i].is_stop,\n",
    "            'is_alpha': tokens[i].is_alpha,\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = tokens.text[i-1][0]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(tokens)-1:\n",
    "            word1 = tokens.text[i+1][0]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "\n",
    "        return features\n",
    "    return [single_token2feature(tokens, i) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features\n",
    "### We transform our processed training and testing data into features <br><br> X_train, X_test contains the features, while y_train, y_test only contains the POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [token2features(tokens) for tokens,_ in train]\n",
    "y_train = [pos for _, pos in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [token2features(tokens) for tokens,_ in test]\n",
    "y_test = [pos for _, pos in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### We use the sklearn_crfsuite.CRF for training, the parameters supplied to the function (algorithm, c1, c2, max_iterations and  all_possible_transitions) are hyperparameters which we can use for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the classes of the model\n",
    "## Here we should see the POS tags that our model can predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "### We used X_train, y_train for training the model, now we will use X_test, y_test for testing the model. To do the testing first we predict the POS tags of X_test with our model, then we compare the predicted POS tags to the ground truth POS tags of y_test using the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on custom text\n",
    "## Here we can try our model on some custom text\n",
    "## First let's define some helper functions that will help us in the transformation of the input text into a format that our model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(custom):\n",
    "    if isinstance(custom, str):  \n",
    "        tokenized = nlp(custom)\n",
    "        postags = [token.pos_ for token in tokenized]\n",
    "        return tokenized, postags\n",
    "    else:\n",
    "        postags = [token.pos_ for token in custom]\n",
    "        return custom, postags\n",
    "\n",
    "def pos_tagger(custom):\n",
    "    tokens, postags = pre_process(custom)\n",
    "    features = token2features(tokens)\n",
    "    return crf.predict([features]), postags        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_custom(custom):\n",
    "    predict, true = pos_tagger(custom)\n",
    "\n",
    "    predict = predict[0] # Remove from inner list\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(predict)):\n",
    "        if(predict[i] == true[i]):\n",
    "            correct += 1\n",
    "\n",
    "    print(f\"Predicted: {predict}\")\n",
    "    print(f\"True: {true}\")\n",
    "    print(f\"Accuracy: {correct}/{len(predict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can use string or a list of tokens as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_custom(\"Hello this is a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(\"Hello this is a test\")\n",
    "evaluate_custom(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try something (type your text here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_custom(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
