{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Kevin_Hartyányi_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78_8yilwnK1"
      },
      "source": [
        "# Importing the libraries that the program needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUUHqgRownLD",
        "outputId": "85bc1156-2cd8-4151-d139-cbe8e8e0f67b"
      },
      "source": [
        "import sys\n",
        "print(\"Python:\", sys.version)\n",
        "\n",
        "import numpy as np\n",
        "print(\"NumPy:\", np.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "\n",
        "import sklearn\n",
        "print(\"Scikit-learn:\", sklearn.__version__)\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.attrs import IS_TITLE, LOWER, IS_ALPHA, IS_UPPER, IS_DIGIT\n",
        "print(\"SpaCy:\", spacy.__version__)\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python: 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "NumPy: 1.19.5\n",
            "Pandas: 1.1.5\n",
            "Scikit-learn: 0.22.2.post1\n",
            "SpaCy: 2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBWCzsgLwnLH"
      },
      "source": [
        "# Downloading the brown corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxnSKpy_wnLH",
        "outputId": "e1dc6d22-3590-4c4c-d2ec-94d22cd2d4ce"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnkQ-OQ-wnLI"
      },
      "source": [
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKbjM3kLPkM4"
      },
      "source": [
        "# We load the words from the corpus and while we iterate over them we also filter with the \"isalpha\" method. By using this method we can remove characters like !#%&?, which we won't need during the training, because this characters don't have much meaning in our case.\n",
        "\n",
        "# We also transform the words into lower case format, so we can avoid having duplicates of the same word like \"The\" and \"the\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj60dsmKwnLJ",
        "outputId": "bdd68e70-c496-4c00-d1f1-059f2c647ac4"
      },
      "source": [
        "words = [word.lower() for word in brown.words() if word.isalpha()]\n",
        "words[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'fulton',\n",
              " 'county',\n",
              " 'grand',\n",
              " 'jury',\n",
              " 'said',\n",
              " 'friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nWss8G-Qe5D"
      },
      "source": [
        "# In the next step we transform the list of words into partial sentences of 6 word length. Basically our model will learn to predict the next word based on the previous 5 words.\n",
        "\n",
        "# While we are creating the partial sequences we also do another filtering where we skip those partial sequences where one of the words is a non frequent one. A word is frequent if it appears at least 10 times in the corpus. This will help us to train the model, because it's hard to learn a non frequent word, given that it doesn't appear often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGFqT_LgwnLL"
      },
      "source": [
        "frequency = nltk.FreqDist(w for w in words)\n",
        "frequent_words = set(map(lambda x: x[0], filter(lambda x: x[1] >= 10, frequency.items())))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5k2aMp2iJ5"
      },
      "source": [
        "def contains_rare_word(sent):\n",
        "  return any([word not in frequent_words for word in sent])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIHXZkEIwnLQ",
        "outputId": "76209fa0-ca96-4b71-9cf8-b1b317f9e91e"
      },
      "source": [
        "length = 5 + 1\n",
        "partial_sents = list()\n",
        "for i in range(length, len(words)):\n",
        "    word_seq = words[i-length:i]\n",
        "    if contains_rare_word(word_seq):\n",
        "      continue\n",
        "    sentence = ' '.join(word_seq)\n",
        "    partial_sents.append(sentence)\n",
        "print(\"Total sentences: \", len(partial_sents))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences:  610583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxT-KAuqR1Lz"
      },
      "source": [
        "# The word embedding layer expects integers as it's input, so we need to transform our data.\n",
        "\n",
        "# To do this we need to map our words to a unique integer and encode our input sequences. When the model makes a prediction, we can convert it back to a word using the same mapping.\n",
        "\n",
        "# We use the keras Tokenizer to do this which we first train on the dataset, where it learns all of the unique words and assigns a unique integer to each of them, then we can use this trained Tokenizer to transform a list of words into a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJr9yshPwnLT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(partial_sents)\n",
        "sents = tokenizer.texts_to_sequences(partial_sents)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M83eE7tkwnLT",
        "outputId": "f8c54174-d7d3-4851-8264-0c3e6a2ef207"
      },
      "source": [
        "sents[:3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 4527, 767, 2762, 1514, 54],\n",
              " [4527, 767, 2762, 1514, 54, 1921],\n",
              " [767, 2762, 1514, 54, 1921, 33]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4GE09uqToVv"
      },
      "source": [
        "# We can acces the mapping with the word_index attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_15KZIScwnLU",
        "outputId": "61a9c223-69af-4844-a164-244c888a6e3f"
      },
      "source": [
        "unique_words = len(tokenizer.word_index) + 1\n",
        "print(\"Total words: \", len(words))\n",
        "print(\"Unique words after filtering: \", unique_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words:  981716\n",
            "Unique words after filtering:  8144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGgbN-nDT4p8"
      },
      "source": [
        "# Now the we have our encoded sentences we need to split them into X and y where X is the input and y is the expected output.\n",
        "\n",
        "# We also need to one-hot encode the output words, because this way the model will learn to predict the probability distribution of the next word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Ow8GHQwnLV"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "temp_array = np.array(sents)\n",
        "X, y = temp_array[:,:-1], temp_array[:,-1]\n",
        "y = to_categorical(y, num_classes=unique_words, dtype=np.uint8)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o81H8uxBVg1E",
        "outputId": "7f58a514-8032-4943-8b6d-798e14a7e185"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(610583, 8144)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-0rWi_pU-WX"
      },
      "source": [
        "# Now we can define our model. We provide the number of unique words and the length of our input sentences to the embedding layer and choose 100 for the output dimension.\n",
        "\n",
        "# We will use two LSTM layers, with more layers we might achieve better results, but the training time would increase.\n",
        "\n",
        "# On the last LSTM layer we use the return_state=True, so it will return the cell state\n",
        "\n",
        "# At the end we have two Dense layers, where the final Dense layer predicts the next word as a probability related to each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8s9g0qd-RJ",
        "outputId": "d1230e1a-89f0-498d-b490-f1379d2284fa"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow import keras\n",
        "\n",
        "inputs = keras.Input(shape=(X.shape[1],))\n",
        "m = Embedding(unique_words, 10, input_length=X.shape[1])(inputs)\n",
        "#m = LSTM(100, return_sequences=True)(m)\n",
        "m,_,_ = LSTM(100, return_state=True)(m)\n",
        "outputs = Dense(unique_words, activation='softmax')(m)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"language_model\")\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"language_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 5, 10)             81440     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 100), (None, 100) 44400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8144)              822544    \n",
            "=================================================================\n",
            "Total params: 948,384\n",
            "Trainable params: 948,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ysoTEDIYFQY"
      },
      "source": [
        "# Because the model could be interpreted as learning a multiclass classification we use categorical cross entropy.\n",
        "\n",
        "# For the optimizer we use Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhk70W6dwnLX",
        "outputId": "e36e7fab-178a-447a-ee25-b2570ad691e1"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=64, epochs=5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "9541/9541 [==============================] - 91s 6ms/step - loss: 6.6302 - accuracy: 0.0911\n",
            "Epoch 2/5\n",
            "9541/9541 [==============================] - 57s 6ms/step - loss: 5.8933 - accuracy: 0.1360\n",
            "Epoch 3/5\n",
            "9541/9541 [==============================] - 57s 6ms/step - loss: 5.6328 - accuracy: 0.1502\n",
            "Epoch 4/5\n",
            "9541/9541 [==============================] - 57s 6ms/step - loss: 5.4752 - accuracy: 0.1589\n",
            "Epoch 5/5\n",
            "9541/9541 [==============================] - 57s 6ms/step - loss: 5.3707 - accuracy: 0.1637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9ed41a3ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDKISz1_Yt5x"
      },
      "source": [
        "# Generating words\n",
        "\n",
        "# Here we define a function that we can use to generate an arbitrary long text. To do this we need a seeding text as an input that we first encode using the Tokenizer, then we truncate it to have 5 word length, because our model learned on 5 word long inputs. After predicting the index of the next word using the model, we lookup the corresponding word. Finally, we append it into the end of our seed text and repeat the process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S05siXeL_hb3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def int_to_word(predicted):\n",
        "  pred_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      pred_word = word\n",
        "      break\n",
        "  return pred_word\n",
        "\n",
        "def generate_text(model, seed_text, generate_num, test=False):\n",
        "  generated = list()\n",
        "  text = seed_text\n",
        "  input_len = length - 1\n",
        "\n",
        "  for i in range(generate_num):\n",
        "    if test:\n",
        "      print(\"Text:\", text)\n",
        "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
        "    if test:\n",
        "      print(\"Encoded:\", encoded)\n",
        "    encoded = pad_sequences([encoded], maxlen=input_len, truncating='pre')\n",
        "    if test:\n",
        "      print(\"Padded:\", encoded)\n",
        "    pred_probability = model.predict(encoded, verbose=0)\n",
        "    pred = np.argmax(pred_probability,axis=1)\n",
        "    pred_word = int_to_word(pred)\n",
        "    text += ' ' + pred_word\n",
        "    generated.append(pred_word)\n",
        "  return ' '.join(generated)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OXpDnGJBc13",
        "outputId": "f6d464aa-5e42-45cb-d70d-87d39bde313e"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "seed_text = partial_sents[randint(0,len(partial_sents))]\n",
        "\n",
        "generated = generate_text(model, seed_text, 5)\n",
        "print(\"Start text:\", seed_text)\n",
        "print(\"Generate text:\", generated)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start text: should be substantially reduced and ultimately\n",
            "Generate text: as the result of the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wRKBqBHdx-f"
      },
      "source": [
        "# Custom input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpXa012aBjeO"
      },
      "source": [
        "def custom_input(text):\n",
        "  tokenized = nlp(text)\n",
        "  pre_process = ' '.join([word.text.lower() for word in tokenized])\n",
        "  return generate_text(model, pre_process, 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYnnrSDqd9bE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fe0d9c-9fe3-4bf3-86a3-1150a0f07954"
      },
      "source": [
        "print(\"Exit with ''(enter)\")\n",
        "start_text = input()\n",
        "while start_text != \"\":\n",
        "  next_word = custom_input(start_text)\n",
        "  print(\"Next word:\", next_word)\n",
        "  start_text = input()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exit with ''(enter)\n",
            " \n",
            "Next word: and\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l58mj2nFX36G"
      },
      "source": [
        "# Search Engine Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpqV2QLNpKrt"
      },
      "source": [
        "# Define the helper functions to get the LSTM cell state. The LSTM layer is the 3rd one, that's why we use that layer as our output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo1ZN8UXYe41"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "last_lstm = K.function([model.layers[0].input], \n",
        "                       [model.layers[2].output])\n",
        "\n",
        "def get_cell_state(word_id_vector):\n",
        "  return last_lstm([word_id_vector])[0][0][2][0]\n",
        "\n",
        "def get_cell_state_from_sentence(sent):\n",
        "  encoded = tokenizer.texts_to_sequences([sent])[0]\n",
        "  encoded = pad_sequences([encoded], maxlen=input_len, truncating='pre')\n",
        "  return get_cell_state(encoded)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwYD_zxVeyXn"
      },
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlJ47Y6jpeHp"
      },
      "source": [
        "# Transforms a sentence into a vector by using the trained model and extracting the cell state, then compares it with another vector to get the cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjVKFKSJYXTg"
      },
      "source": [
        "def sentence_cosine_similarity(a,b):\n",
        "    input_len = length - 1\n",
        "    \n",
        "    encoded_a = tokenizer.texts_to_sequences([a])[0]\n",
        "    encoded_a = pad_sequences([encoded_a], maxlen=input_len, truncating='pre')\n",
        "    a_cell_state = get_cell_state(encoded_a)\n",
        "\n",
        "    encoded_b = tokenizer.texts_to_sequences([b])[0]\n",
        "    encoded_b = pad_sequences([encoded_b], maxlen=input_len, truncating='pre')\n",
        "    b_cell_state = get_cell_state(encoded_b)\n",
        "\n",
        "    return cosine_similarity(a_cell_state,b_cell_state)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIqWFQPW5MBm"
      },
      "source": [
        "# Custom sentence similarities (Cosine similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844Fyjh1kti4",
        "outputId": "8c5f39fc-2b0a-4dee-c6e7-76efa827433d"
      },
      "source": [
        "a = input()\n",
        "b = input()\n",
        "similarity = sentence_cosine_similarity(a,b)\n",
        "print(\"Sentence 1:\", a)\n",
        "print(\"Sentence 2:\", b)\n",
        "print(\"Similarity:\", similarity)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "Hi\n",
            "Sentence 1: Hello\n",
            "Sentence 2: Hi\n",
            "Similarity: 0.92856324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6YpD12r5rIg"
      },
      "source": [
        "# Mini search engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7F5Nu8Ykx8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40cec44-a77a-42b1-81c3-d503e04be8e8"
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 8.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391640 sha256=85f8340d56071509a97b06cf844fa909cd64101b0d80f06cd8b7f3d9012c71ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWa4SOTp6jmR"
      },
      "source": [
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbEaBjpy88sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cc1f29-8ecd-4dfd-e387-d406b4f4759e"
      },
      "source": [
        "len(brown.sents())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMBwQrYCsS5e"
      },
      "source": [
        "# We use annoy to index all of the sentences in the brown corpus. First we transform the sentence into a vector by passing it through our model and taking the cell state representation of the lstm layer, then we pass this vector into annoy to store it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFtrvSFZ8Nx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6255fb-f866-419d-a60f-a01f6e8df709"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "f = 100\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "input_len = length - 1\n",
        "\n",
        "with tqdm.tqdm(total=len(brown.sents())) as pbar:\n",
        "  for i, sent in enumerate(brown.sents()):  \n",
        "    encoded = tokenizer.texts_to_sequences([sent])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=input_len, truncating='pre')\n",
        "    cell_state = get_cell_state(encoded)\n",
        "    t.add_item(i, cell_state)\n",
        "    pbar.update(1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 57340/57340 [04:14<00:00, 225.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgiK04UCu8_0"
      },
      "source": [
        "# We build the trees which will be used for querying. With more trees we can get a higher precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nw7bc1fuxOT",
        "outputId": "4af20465-9edd-4c61-f676-d3230abeafdf"
      },
      "source": [
        "t.build(10)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N65L7SNds7nP"
      },
      "source": [
        "# Testing with a sentence. Annoy returns the 5 closest neighbours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzh-W3MwvOM"
      },
      "source": [
        "def get_sentence(index):\n",
        "  return brown.sents()[index]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USymEEbc-uTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0776d54c-8f95-4a9b-9991-8bbcac7ae1a0"
      },
      "source": [
        "search = \"this is a test\"\n",
        "cell_state = get_cell_state_from_sentence(search)\n",
        "neighbours = t.get_nns_by_vector(cell_state, 5)\n",
        "for nn in neighbours:\n",
        "  print(get_sentence(nn))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'chase', 'in', 'itself', 'is', 'a', 'narrative', ';', ';']\n",
            "['This', 'is', 'a', 'mistake', '.']\n",
            "['And', 'the', '100,000', 'subscribers', 'became', 'a', 'reality', '.']\n",
            "['The', 'question', 'becomes', ',', '``', 'What', 'is', 'a', 'dream', \"''\", '?', '?']\n",
            "['``', 'There', 'must', 'be', 'a', 'line', \"''\", '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxfD65JtypnT"
      },
      "source": [
        "# Custom input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_ZEdr5Hurgj",
        "outputId": "bf412988-70d5-4dc8-e05e-a4f5ec2853b4"
      },
      "source": [
        "print(\"Exit with ''(enter)\")\n",
        "search = input()\n",
        "while search != \"\":\n",
        "  cell_state = get_cell_state_from_sentence(search)\n",
        "  neighbours = t.get_nns_by_vector(cell_state, 5)\n",
        "  for nn in neighbours:\n",
        "    print(get_sentence(nn))\n",
        "  search = input()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exit with ''(enter)\n",
            "Hello how are you?\n",
            "['``', 'Hello', ',', 'Julie', ',', 'how', 'are', 'you', \"''\", '?', '?']\n",
            "['If', 'he', 'bites', 'a', 'playmate', 'she', 'says', ',', '``', 'Danny', \"won't\", 'like', 'you', \"''\", '.']\n",
            "['It', \"wouldn't\", 'matter', 'to', 'a', 'fool', 'like', 'you', '.']\n",
            "['If', 'you', 'have', 'an', 'annual', 'or', 'regular', 'physical', 'examination', 'program', ',', 'is', 'it', 'worth', 'what', 'it', 'is', 'costing', 'you', '?', '?']\n",
            "['``', \"I'd\", 'give', 'anything', 'if', 'I', 'could', 'have', 'found', 'a', 'girl', 'like', 'you', \"''\", '.']\n",
            "Once\n",
            "['Now', '!', '!']\n",
            "['Now', '!', '!']\n",
            "['``', 'Now', \"''\", '?', '?']\n",
            "['Now', '!', '!']\n",
            "['Then']\n",
            " \n",
            "['(', '6', ')']\n",
            "['Sec.', '2', '.']\n",
            "['Mr.', 'Stratton', '.']\n",
            "[')', '9', '.']\n",
            "['2', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFjkGKKsvrbD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}