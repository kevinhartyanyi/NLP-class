{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "NumPy: 1.16.4\n",
      "Pandas: 0.25.3\n",
      "Scikit-learn: 0.23.2\n",
      "SpaCy: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"Scikit-learn:\", sklearn.__version__)\n",
    "\n",
    "import spacy\n",
    "print(\"SpaCy:\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the SMS Spam Collections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y                                                  x\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SMSSpamCollection.txt', sep='\\t',names= ['y', 'x'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           y                       x\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2bf0d53d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['x'].apply(len).plot(bins=50, kind='hist') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and POS-tag the input texts with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Old: df['tokens'] = df['x'].apply(nlp)\n",
    "tokens = []\n",
    "for text in nlp.pipe(iter(df['x']), n_threads=-1):\n",
    "    tokens.append(text)\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>(Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>(Ok, lar, ..., Joking, wif, u, oni, ...)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>(Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>(U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>(Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>(This, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>(Will, ü, b, going, to, esplanade, fr, home, ?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>(Pity, ,, *, was, in, mood, for, that, ., So, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>(The, guy, did, some, bitching, but, I, acted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>(Rofl, ., Its, true, to, its, name)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y                                                  x  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham               Will ü b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0     (Go, until, jurong, point, ,, crazy, .., Avail...  \n",
       "1              (Ok, lar, ..., Joking, wif, u, oni, ...)  \n",
       "2     (Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3     (U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
       "4     (Nah, I, do, n't, think, he, goes, to, usf, ,,...  \n",
       "...                                                 ...  \n",
       "5567  (This, is, the, 2nd, time, we, have, tried, 2,...  \n",
       "5568    (Will, ü, b, going, to, esplanade, fr, home, ?)  \n",
       "5569  (Pity, ,, *, was, in, mood, for, that, ., So, ...  \n",
       "5570  (The, guy, did, some, bitching, but, I, acted,...  \n",
       "5571                (Rofl, ., Its, true, to, its, name)  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>(Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>(Ok, lar, ..., Joking, wif, u, oni, ...)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>(Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>(U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>(Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y                                                  x  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  (Go, until, jurong, point, ,, crazy, .., Avail...  \n",
       "1           (Ok, lar, ..., Joking, wif, u, oni, ...)  \n",
       "2  (Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  (U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
       "4  (Nah, I, do, n't, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-tag\n",
    "df['pos'] = df['tokens'].apply(lambda x: [i.pos_ for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>(Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[VERB, ADP, ADJ, NOUN, PUNCT, ADJ, PUNCT, ADJ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>(Ok, lar, ..., Joking, wif, u, oni, ...)</td>\n",
       "      <td>[INTJ, NOUN, PUNCT, NOUN, CCONJ, PROPN, PROPN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>(Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[ADJ, NOUN, ADP, NUM, DET, NOUN, NOUN, PART, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>(U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "      <td>[PROPN, PROPN, VERB, ADV, ADJ, NOUN, PUNCT, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>(Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[INTJ, PRON, AUX, PART, VERB, PRON, VERB, ADP,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y                                                  x  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  (Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1           (Ok, lar, ..., Joking, wif, u, oni, ...)   \n",
       "2  (Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  (U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
       "4  (Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [VERB, ADP, ADJ, NOUN, PUNCT, ADJ, PUNCT, ADJ,...  \n",
       "1  [INTJ, NOUN, PUNCT, NOUN, CCONJ, PROPN, PROPN,...  \n",
       "2  [ADJ, NOUN, ADP, NUM, DET, NOUN, NOUN, PART, V...  \n",
       "3  [PROPN, PROPN, VERB, ADV, ADJ, NOUN, PUNCT, PR...  \n",
       "4  [INTJ, PRON, AUX, PART, VERB, PRON, VERB, ADP,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do POS-tag based and list-based stop-word filtering using ​ spaCy​ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop word filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_worlds(tokens):\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if not t.is_stop:\n",
    "            filtered_tokens.append(t)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [jurong, point, ,, crazy, .., Available, bugis...\n",
       "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
       "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3                    [U, dun, early, hor, ..., U, c, ...]\n",
       "4                       [Nah, think, goes, usf, ,, lives]\n",
       "                              ...                        \n",
       "5567    [2nd, time, tried, 2, contact, u., U, won, £, ...\n",
       "5568                [ü, b, going, esplanade, fr, home, ?]\n",
       "5569           [Pity, ,, *, mood, ., ..., suggestions, ?]\n",
       "5570    [guy, bitching, acted, like, interested, buyin...\n",
       "5571                                      [Rofl, ., true]\n",
       "Name: tokens, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_filtered = df['tokens'].apply(lambda x: filter_stop_worlds(x))\n",
    "stop_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train 3 classical ML-based classifiers on TF-IDF BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# use lambda x: x to disable the built in tokenizer of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None) \n",
    "\n",
    "X = vectorizer.fit_transform(stop_filtered)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 62957)\n",
      "(4457,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663677130044843"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "svc = svm.LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1336322869955157"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.toarray(), y_train)\n",
    "gnb.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663677130044843"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords_and_get_lemma(tokens):\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if not t.is_stop:\n",
    "            filtered_tokens.append(t.lemma)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14096818269429646414,\n",
       " 15479733260938818482,\n",
       " 2593208677638477497,\n",
       " 16792598155547231470,\n",
       " 8848021949395737739,\n",
       " 4887332976578131782,\n",
       " 3557720637082594454,\n",
       " 13210364986222294696,\n",
       " 8881679497796027013,\n",
       " 1703489418272052182,\n",
       " 6804705863737483857,\n",
       " 1720370409040345145,\n",
       " 9503240487077079950,\n",
       " 10875615029400813363,\n",
       " 3040539950272399101,\n",
       " 2013399242189103424,\n",
       " 15198862986666964157,\n",
       " 8445541832527975875,\n",
       " 10875615029400813363]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_filtered_lemma = df['tokens'].apply(lambda x: filter_stopwords_and_get_lemma(x))\n",
    "stop_filtered_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# use lambda x: x to disable the built in tokenizer of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None) \n",
    "\n",
    "X = vectorizer.fit_transform(stop_filtered_lemma)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838565022421525"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "svc = svm.LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8887892376681614"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.toarray(), y_train)\n",
    "gnb.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713004484304932"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                   Results\n",
      "                     With text                With lemma\n",
      "SVM               0.8663677130044843      0.8663677130044843\n",
      "Naive Bayes       0.1336322869955157      0.7614349775784753\n",
      "Random Forest     0.8663677130044843      0.9076233183856502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "                                   Results\n",
    "                     With text                With lemma\n",
    "SVM               0.8663677130044843      0.9838565022421525\n",
    "Naive Bayes       0.1336322869955157      0.8887892376681614\n",
    "Random Forest     0.8663677130044843      0.9713004484304932\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
